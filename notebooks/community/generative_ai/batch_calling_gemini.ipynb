{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Batch calling Gemini\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/generative_ai/batch_calling_gemini.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fcommunity%2Fgenerative_ai%2Fbatch_calling_gemini.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/community/generative_ai/batch_calling_gemini.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/generative_ai/batch_calling_gemini.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "This notebook provides examples of how to use Gemini for batch workloads asynchronously. It covers both the Gemini online API and the Gemini Batch API.\n",
    "\n",
    "This tutorial uses the following Google Cloud services:\n",
    "\n",
    "- Gemini on Vertex AI\n",
    "- Batch Gemini API on Vertex AI\n",
    "- BigQuery\n",
    "- Google Cloud Storage\n",
    "\n",
    "The steps performed include:\n",
    "\n",
    "- Installation and imports\n",
    "- Calling Gemini online (synchronous calls)\n",
    "- Building a sample dataset in JSONL format\n",
    "- Calling Gemini asynchronously\n",
    "- Using the Gemini Batch API via GCS\n",
    "- Using the Gemini Batch API via BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4e3e949c0bdd"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Google Cloud Storage\n",
    "* BigQuery\n",
    "\n",
    "Learn about [Google Cloud pricing](https://cloud.google.com/pricing/list?hl=en) and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "**Important:** This notebook sends a large number of API calls to Gemini for inference. To limit costs, reduce the size of the test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Vertex AI SDK for Python and other required packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet google-cloud-aiplatform google-cloud-bigquery google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R5Xep4W9lq-Z"
   },
   "source": [
    "### Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRvKdaPDTznN"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SbmM4z7FOBpM"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmWOrTJ3gx13"
   },
   "source": [
    "### Authenticate your notebook environment (Colab only)\n",
    "\n",
    "Authenticate your environment on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NyKGtVQjgx13"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    from google.colab import auth\n",
    "\n",
    "    auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "### Set Google Cloud project information and initialize Vertex AI SDK for Python\n",
    "\n",
    "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "import vertexai\n",
    "\n",
    "from vertexai.generative_models import GenerativeModel, Part, GenerationConfig\n",
    "from vertexai.batch_prediction import BatchPredictionJob\n",
    "from tenacity import retry, wait_random_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VARIABLES SET BY USER\n",
    "PROJECT_ID = \"multi-tenancy-dataproc\" # @param {type:\"string\"}\n",
    "DEFAULT_MODEL_NAME = \"gemini-1.5-flash-002\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
    "batch_file_name = 'gemini_batch.jsonl' # @param {type:\"string\"}\n",
    "\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synchronous API Calls to Gemini\n",
    "\n",
    "The default and most common way to interact with Gemini is by making synchronous API calls to the online endpoint. This approach allows you to send one input request at a time, which is not ideal for batch workloads.\n",
    "\n",
    "Gemini on Vertex AI offers several optional parameters that allow you to fine-tune the model's output and tailor it to your specific needs. These parameters include:\n",
    "\n",
    "* System instructions\n",
    "* Security filters\n",
    "* Generation config\n",
    "* Metadata labels\n",
    "\n",
    "For examples of how to use these parameters, see the [Intro to Gemini notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_gemini_python.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini inference time with a single prompt: 7.4 seconds.\n",
      "\n",
      "The image contains a tabby cat.\n",
      "\n",
      "The video shows the following animals:\n",
      "\n",
      "- Giraffes\n",
      "- A Sumatran tiger\n",
      "- Asian elephants\n",
      "- Giant otters\n",
      "- A sloth\n",
      "- Dik-diks\n"
     ]
    }
   ],
   "source": [
    "# Basic API call to Gemini\n",
    "\n",
    "# Initialize the multimodal model with system instructions.\n",
    "# This tells Gemini to act as a zoologist whose mission is to find and rescue animals.\n",
    "multimodal_model = GenerativeModel(\n",
    "    model_name=DEFAULT_MODEL_NAME,  # Use the default Gemini model\n",
    "    system_instruction=[\n",
    "        \"You are a zoologist\",\n",
    "        \"Your mission is to find and rescue animals\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the prompt for Gemini.\n",
    "prompt = \"Tell me what animals are in the image and which ones in the video\"\n",
    "\n",
    "# Create parts for the video and image.\n",
    "# These parts point to the respective media files in Google Cloud Storage.\n",
    "video_part = Part.from_uri(\n",
    "    \"gs://cloud-samples-data/video/animals.mp4\", \n",
    "    mime_type=\"video/mp4\"\n",
    ")\n",
    "image_part = Part.from_uri(\n",
    "    \"gs://download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg\",\n",
    "    mime_type=\"image/jpeg\",\n",
    ")\n",
    "\n",
    "# API Call\n",
    "start_time = time.time()  # Record the start time for measuring inference time\n",
    "\n",
    "# Send the prompt and media parts to Gemini for processing.\n",
    "response = multimodal_model.generate_content(\n",
    "    contents=[\n",
    "        prompt,\n",
    "        video_part,\n",
    "        image_part,\n",
    "    ],\n",
    "    # Configure generation parameters like temperature, top_p, top_k, etc.\n",
    "    generation_config=GenerationConfig(\n",
    "        temperature=0.9,\n",
    "        top_p=1.0,\n",
    "        top_k=32,\n",
    "        candidate_count=1,\n",
    "        max_output_tokens=8192,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Print the response.\n",
    "print(f\"Gemini inference time with a single prompt: {time.time() - start_time:.1f} seconds.\\n\")\n",
    "print(response.text)  # Output the text generated by Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gemini Batch API requires datasets to be in JSONL format with a specific structure.  A [Sample JSONL file is available on GCS](https://storage.googleapis.com/cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl)  but we build a separate sample JSONL dataset for all the tests in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'request': {'contents': [{'role': 'user',\n",
       "    'parts': [{'text': 'Here goes the prompt.'},\n",
       "     {'file_data': {'file_uri': 'Here goes the UCS URI with the image or video',\n",
       "       'mime_type': 'image/jpeg'}}]}],\n",
       "  'generationConfig': {'temperature': 0.4},\n",
       "  'system_instruction': {'parts': [{'text': 'You are an expert in math'}]}},\n",
       " 'metadata': 'prompt identifier'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each single line of the JSONL is structured like this:\n",
    "baseline = '''\n",
    "{\n",
    "    \"request\": {\n",
    "        \"contents\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"text\": \"Here goes the prompt.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"file_data\": {\n",
    "                            \"file_uri\": \"Here goes the UCS URI with the image or video\",\n",
    "                            \"mime_type\": \"image/jpeg\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"generationConfig\": {\n",
    "            \"temperature\": 0.4\n",
    "        },\n",
    "        \"system_instruction\": {\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"text\": \"You are an expert in math\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"metadata\": \"prompt identifier\"\n",
    "}\n",
    "'''\n",
    "jsonl_baseline_dict = json.loads(baseline)\n",
    "jsonl_baseline_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To speed up the tests on this notebook, we remove the image and use only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'request': {'contents': [{'role': 'user',\n",
       "    'parts': [{'text': 'Here goes the prompt.'}]}],\n",
       "  'generationConfig': {'temperature': 0.4},\n",
       "  'system_instruction': {'parts': [{'text': 'You are an expert in math'}]}},\n",
       " 'metadata': 'prompt identifier'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing image part from baseline\n",
    "del jsonl_baseline_dict['request']['contents'][0]['parts'][1]\n",
    "jsonl_baseline_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Helper functions to create a dataset using the baseline dictionary and to call Gemini synchronously with only text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_sample_dataset(size=10, batch_file_name=batch_file_name):\n",
    "    \"\"\"\n",
    "    Creates a sample dataset of math problems and saves it to a JSONL file.\n",
    "\n",
    "    Args:\n",
    "      size: The number of problems to generate (default: 20).\n",
    "      batch_file_name: The base name for the output file.\n",
    "\n",
    "    Returns:\n",
    "      A pandas DataFrame containing the generated dataset.\n",
    "    \"\"\"\n",
    "    file_name = f\"{str(size)}_prompt_{batch_file_name}\"\n",
    "    gemini_batch_jsonl = open(file_name, 'w')\n",
    "    \n",
    "    for i in range(size):\n",
    "        # Generate a math problem in the form \"a + b x c\"\n",
    "        prompt = f\"Print the result of the following equation and explain the result: {i} + {str(random.randint(0,10))} x {str(random.randint(0,1000))}\"\n",
    "        \n",
    "        # 'jsonl_baseline_dict' is a predefined dictionary with the basic structure for the JSONL file\n",
    "        temp_dict = dict(jsonl_baseline_dict)  \n",
    "        \n",
    "        # Update the 'text' field with the generated prompt\n",
    "        temp_dict['request']['contents'][0]['parts'][0]['text'] = prompt  \n",
    "        \n",
    "        # Update the 'temperature' with a random value betweeen 0 and 2\n",
    "        temp_dict['request']['generationConfig']['temperature'] = round(random.uniform(0, 2), 1)\n",
    "        \n",
    "        # Add metadata to the dictionary\n",
    "        temp_dict['metadata'] = f'row_id_{str(i)}'\n",
    "        \n",
    "        # Write the dictionary to the JSONL file\n",
    "        gemini_batch_jsonl.write(json.dumps(temp_dict)+'\\n')\n",
    "\n",
    "    # Close the file\n",
    "    gemini_batch_jsonl.close()\n",
    "    print(f\"JSONL file created: {file_name}\")\n",
    "\n",
    "    # Read the JSONL file into a pandas DataFrame\n",
    "    sample_dataset_df = pd.read_json(file_name, lines=True)\n",
    "    return sample_dataset_df, file_name\n",
    "\n",
    "\n",
    "@retry(wait=wait_random_exponential(multiplier=1, max=20))\n",
    "def pandas_call_gemini(row):\n",
    "    \"\"\"\n",
    "    Calls the Gemini model with data from a pandas DataFrame row.\n",
    "\n",
    "    This function is designed to be used with pandas' apply method to process\n",
    "    rows in a DataFrame. It handles potential transient errors by retrying the \n",
    "    Gemini API call with exponential backoff.\n",
    "\n",
    "    Args:\n",
    "        row: A pandas DataFrame row containing the following columns:\n",
    "            - 'system_instruction': A dictionary with a 'parts' key containing a list of dictionaries, \n",
    "                                    where the first dictionary has a 'text' key with the system instruction.\n",
    "            - 'contents': A dictionary with a 'parts' key containing a list of dictionaries,\n",
    "                          where the first dictionary has a 'text' key with the content for the model.\n",
    "            - 'generationConfig': A dictionary with the generation configuration for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The text response from the Gemini model.\n",
    "    \"\"\"\n",
    "    multimodal_model = GenerativeModel(DEFAULT_MODEL_NAME, system_instruction=[row['system_instruction']['parts'][0]['text']])\n",
    "    response = multimodal_model.generate_content(contents=[row['contents'][0]['parts'][0]['text']], generation_config=row['generationConfig'])\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Synchronous test\n",
    "\n",
    "* Creating a sample dataset\n",
    "* Call gemini row by row synchronously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 20_prompt_gemini_batch.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              request   metadata\n",
       "0   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_0\n",
       "1   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_1\n",
       "2   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_2\n",
       "3   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_3\n",
       "4   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_4\n",
       "5   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_5\n",
       "6   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_6\n",
       "7   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_7\n",
       "8   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_8\n",
       "9   {'contents': [{'role': 'user', 'parts': [{'tex...   row_id_9\n",
       "10  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_10\n",
       "11  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_11\n",
       "12  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_12\n",
       "13  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_13\n",
       "14  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_14\n",
       "15  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_15\n",
       "16  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_16\n",
       "17  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_17\n",
       "18  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_18\n",
       "19  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_19"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a sample dataset\n",
    "sample_dataset_df, file_name = create_sample_dataset(size=20)\n",
    "sample_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini inference time with 20 prompts: 16.6 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_0</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_2</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_3</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             request  metadata  \\\n",
       "0  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_0   \n",
       "1  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_1   \n",
       "2  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_2   \n",
       "3  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_3   \n",
       "4  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_4   \n",
       "\n",
       "                                     gemini_response  \n",
       "0  Following the order of operations (PEMDAS/BODM...  \n",
       "1  Following the order of operations (PEMDAS/BODM...  \n",
       "2  Following the order of operations (PEMDAS/BODM...  \n",
       "3  Following the order of operations (PEMDAS/BODM...  \n",
       "4  Following the order of operations (PEMDAS/BODM...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Call the synchronous function for each row in the pandas DF\n",
    "start_time = time.time()\n",
    "sample_dataset_df['gemini_response'] = sample_dataset_df.request.apply(pandas_call_gemini)\n",
    "print(f\"Gemini inference time with {len(sample_dataset_df)} prompts: {time.time() - start_time:.1f} seconds\")\n",
    "sample_dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asynchronous test\n",
    "\n",
    "* Define async function and a custom batch function\n",
    "* Call gemini Asynchronously "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(multiplier=1, max=20))\n",
    "async def async_pandas_call_gemini(row):\n",
    "    \"\"\"\n",
    "    Asynchronously calls the Gemini model with data from a pandas DataFrame row.\n",
    "\n",
    "    This function is an asynchronous version of pandas_call_gemini. It's designed \n",
    "    to be used with pandas' apply method in an asynchronous context. It handles \n",
    "    potential transient errors by retrying the Gemini API call with exponential \n",
    "    backoff.\n",
    "\n",
    "    Args:\n",
    "        row: A pandas DataFrame row containing the following columns:\n",
    "            - 'system_instruction': A dictionary with a 'parts' key containing a list of dictionaries, \n",
    "                                    where the first dictionary has a 'text' key with the system instruction.\n",
    "            - 'contents': A dictionary with a 'parts' key containing a list of dictionaries,\n",
    "                          where the first dictionary has a 'text' key with the content for the model.\n",
    "            - 'generationConfig': A dictionary with the generation configuration for the model.\n",
    "\n",
    "    Returns:\n",
    "        str: The text response from the Gemini model.\n",
    "    \"\"\"\n",
    "    multimodal_model = GenerativeModel(DEFAULT_MODEL_NAME, system_instruction=[row['system_instruction']['parts'][0]['text']])\n",
    "    response = await multimodal_model.generate_content_async(contents=[row['contents'][0]['parts'][0]['text']], generation_config=row['generationConfig'])\n",
    "    return response.text\n",
    "\n",
    "\n",
    "async def custom_batch_function_gemini(size):\n",
    "    \"\"\"\n",
    "    Generates a sample dataset of math problems, sends them to Gemini for evaluation, \n",
    "    and measures the inference time.\n",
    "\n",
    "    Args:\n",
    "        size: The number of problems to generate for the dataset.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the problems, Gemini responses, and metadata.\n",
    "    \"\"\"\n",
    "    # Create the sample dataset\n",
    "    sample_dataset_df, file_name = create_sample_dataset(size=size)  \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Create a list of coroutines to call Gemini for each problem\n",
    "    get_gemini_responses = [async_pandas_call_gemini(row['request']) for _, row in sample_dataset_df.iterrows()]\n",
    "\n",
    "    # Execute the coroutines concurrently using asyncio.gather\n",
    "    async_responses = await asyncio.gather(*get_gemini_responses)  \n",
    "\n",
    "    # Add the Gemini responses to the DataFrame\n",
    "    sample_dataset_df['async_gemini_response'] = async_responses  \n",
    "\n",
    "    # Calculate and print the inference time\n",
    "    print(f\"Gemini inference time with {size} prompts: {time.time() - start_time:.1f} seconds\")  \n",
    "\n",
    "    return sample_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Timing Asynchronous tests "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Async call to Gemini with 20 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 20_prompt_gemini_batch.jsonl\n",
      "Gemini inference time with 20 prompts: 1.2 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>async_gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_15</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_16</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_17</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_18</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_19</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              request   metadata  \\\n",
       "15  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_15   \n",
       "16  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_16   \n",
       "17  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_17   \n",
       "18  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_18   \n",
       "19  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_19   \n",
       "\n",
       "                                async_gemini_response  \n",
       "15  Following the order of operations (PEMDAS/BODM...  \n",
       "16  Following the order of operations (PEMDAS/BODM...  \n",
       "17  Following the order of operations (PEMDAS/BODM...  \n",
       "18  Following the order of operations (PEMDAS/BODM...  \n",
       "19  Following the order of operations (PEMDAS/BODM...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async call to Gemini with 20 prompts\n",
    "# Uncomment the lines below to retest\n",
    "# sample_dataset_df = await custom_batch_function_gemini(20)  \n",
    "# sample_dataset_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async call to Gemini with 100 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 100_prompt_gemini_batch.jsonl\n",
      "Gemini inference time with 100 prompts: 2.2 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>async_gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_95</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_96</td>\n",
       "      <td>The result of the equation 96 + 0 x 520 is **9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_97</td>\n",
       "      <td>The result of the equation 97 + 0 x 602 is 97....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_98</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_99</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              request   metadata  \\\n",
       "95  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_95   \n",
       "96  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_96   \n",
       "97  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_97   \n",
       "98  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_98   \n",
       "99  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_99   \n",
       "\n",
       "                                async_gemini_response  \n",
       "95  Following the order of operations (PEMDAS/BODM...  \n",
       "96  The result of the equation 96 + 0 x 520 is **9...  \n",
       "97  The result of the equation 97 + 0 x 602 is 97....  \n",
       "98  Following the order of operations (PEMDAS/BODM...  \n",
       "99  Following the order of operations (PEMDAS/BODM...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async call to Gemini with 100 prompts\n",
    "# Uncomment the lines below to retest\n",
    "# sample_dataset_df = await custom_batch_function_gemini(100)  \n",
    "# sample_dataset_df.tail() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async call to Gemini with 1000 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 1000_prompt_gemini_batch.jsonl\n",
      "Gemini inference time with 1000 prompts: 9.0 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>async_gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_995</td>\n",
       "      <td>The result of the equation 995 + 0 x 803 is **...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_996</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_997</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_998</td>\n",
       "      <td>The result of the equation 998 + 0 x 79 is **9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_999</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               request    metadata  \\\n",
       "995  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_995   \n",
       "996  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_996   \n",
       "997  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_997   \n",
       "998  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_998   \n",
       "999  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_999   \n",
       "\n",
       "                                 async_gemini_response  \n",
       "995  The result of the equation 995 + 0 x 803 is **...  \n",
       "996  Following the order of operations (PEMDAS/BODM...  \n",
       "997  Following the order of operations (PEMDAS/BODM...  \n",
       "998  The result of the equation 998 + 0 x 79 is **9...  \n",
       "999  Following the order of operations (PEMDAS/BODM...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async call to Gemini with 1000 prompts\n",
    "# Uncomment the lines below to retest\n",
    "# sample_dataset_df = await custom_batch_function_gemini(1000)\n",
    "# sample_dataset_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async call to Gemini with 2000 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 2000_prompt_gemini_batch.jsonl\n",
      "Gemini inference time with 2000 prompts: 43.5 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>async_gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1995</td>\n",
       "      <td>The equation 1995 + 7 x 596 follows the order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1996</td>\n",
       "      <td>The result of the equation 1996 + 0 x 80 is **...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1997</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1998</td>\n",
       "      <td>The equation 1998 + 8 x 866 follows the order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1999</td>\n",
       "      <td>The equation 1999 + 9 x 824 follows the order ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                request     metadata  \\\n",
       "1995  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_1995   \n",
       "1996  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_1996   \n",
       "1997  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_1997   \n",
       "1998  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_1998   \n",
       "1999  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_1999   \n",
       "\n",
       "                                  async_gemini_response  \n",
       "1995  The equation 1995 + 7 x 596 follows the order ...  \n",
       "1996  The result of the equation 1996 + 0 x 80 is **...  \n",
       "1997  Following the order of operations (PEMDAS/BODM...  \n",
       "1998  The equation 1998 + 8 x 866 follows the order ...  \n",
       "1999  The equation 1999 + 9 x 824 follows the order ...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async call to Gemini with 2000 prompts\n",
    "# Uncomment the lines below to retest\n",
    "# sample_dataset_df = await custom_batch_function_gemini(2000)\n",
    "# sample_dataset_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async call to Gemini with 5000 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 5000_prompt_gemini_batch.jsonl\n",
      "Gemini inference time with 5000 prompts: 197.7 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>async_gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_0</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_1</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_2</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_3</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_4</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_4995</td>\n",
       "      <td>The equation 4995 + 8 x 396 follows the order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_4996</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_4997</td>\n",
       "      <td>The equation 4997 + 9 x 164 follows the order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_4998</td>\n",
       "      <td>The equation 4998 + 7 x 154 follows the order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_4999</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                request  metadata  \\\n",
       "0     {'contents': [{'role': 'user', 'parts': [{'tex...     row_0   \n",
       "1     {'contents': [{'role': 'user', 'parts': [{'tex...     row_1   \n",
       "2     {'contents': [{'role': 'user', 'parts': [{'tex...     row_2   \n",
       "3     {'contents': [{'role': 'user', 'parts': [{'tex...     row_3   \n",
       "4     {'contents': [{'role': 'user', 'parts': [{'tex...     row_4   \n",
       "...                                                 ...       ...   \n",
       "4995  {'contents': [{'role': 'user', 'parts': [{'tex...  row_4995   \n",
       "4996  {'contents': [{'role': 'user', 'parts': [{'tex...  row_4996   \n",
       "4997  {'contents': [{'role': 'user', 'parts': [{'tex...  row_4997   \n",
       "4998  {'contents': [{'role': 'user', 'parts': [{'tex...  row_4998   \n",
       "4999  {'contents': [{'role': 'user', 'parts': [{'tex...  row_4999   \n",
       "\n",
       "                                  async_gemini_response  \n",
       "0     Following the order of operations (PEMDAS/BODM...  \n",
       "1     Following the order of operations (PEMDAS/BODM...  \n",
       "2     Following the order of operations (PEMDAS/BODM...  \n",
       "3     Following the order of operations (PEMDAS/BODM...  \n",
       "4     Following the order of operations (PEMDAS/BODM...  \n",
       "...                                                 ...  \n",
       "4995  The equation 4995 + 8 x 396 follows the order ...  \n",
       "4996  Following the order of operations (PEMDAS/BODM...  \n",
       "4997  The equation 4997 + 9 x 164 follows the order ...  \n",
       "4998  The equation 4998 + 7 x 154 follows the order ...  \n",
       "4999  Following the order of operations (PEMDAS/BODM...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async call to Gemini with 5000 prompts\n",
    "# Uncomment the lines below to retest\n",
    "# sample_dataset_df = await custom_batch_function_gemini(5000)\n",
    "# sample_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional helper function to chunk the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "async def custom_chunked_batch_function_gemini(size: int, step: int = 1000):\n",
    "    \"\"\"\n",
    "    Processes a dataset in chunks, making asynchronous calls to Gemini for each chunk.\n",
    "\n",
    "    This function generates a sample dataset, divides it into chunks, and processes each chunk\n",
    "    asynchronously using Gemini. The results are then collected and added to the original dataset.\n",
    "\n",
    "    Args:\n",
    "        size (int): The size of the dataset to generate.\n",
    "        step (int, optional): The size of each chunk. Defaults to 1000.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The original dataset with an additional column containing the Gemini responses.\n",
    "    \"\"\"\n",
    "    # Create the sample dataset\n",
    "    sample_dataset_df, file_name = create_sample_dataset(size=size)\n",
    "    all_async_responses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, size, step):\n",
    "        x = i\n",
    "        chunk_df = sample_dataset_df[x : x + step]\n",
    "\n",
    "        get_gemini_responses = [\n",
    "            async_pandas_call_gemini(row[\"request\"]) for _, row in chunk_df.iterrows()\n",
    "        ]\n",
    "        async_responses = await asyncio.gather(*get_gemini_responses)\n",
    "        all_async_responses.extend(async_responses)\n",
    "\n",
    "    sample_dataset_df[\"async_gemini_response\"] = all_async_responses\n",
    "    print(f\"Gemini inference time with {size} prompts: {time.time() - start_time:.1f} seconds\")\n",
    "    return sample_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async call to Gemini with 5000 prompts in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 5000_prompt_gemini_batch.jsonl\n",
      "Gemini inference time with 5000 prompts: 45.6 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>async_gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4995</td>\n",
       "      <td>The equation 4995 + 3 x 768 follows the order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4996</td>\n",
       "      <td>The equation 4996 + 6 x 370 follows the order ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4997</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4998</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4999</td>\n",
       "      <td>The equation 4999 + 4 x 138 follows the order ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                request     metadata  \\\n",
       "4995  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_4995   \n",
       "4996  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_4996   \n",
       "4997  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_4997   \n",
       "4998  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_4998   \n",
       "4999  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_4999   \n",
       "\n",
       "                                  async_gemini_response  \n",
       "4995  The equation 4995 + 3 x 768 follows the order ...  \n",
       "4996  The equation 4996 + 6 x 370 follows the order ...  \n",
       "4997  Following the order of operations (PEMDAS/BODM...  \n",
       "4998  Following the order of operations (PEMDAS/BODM...  \n",
       "4999  The equation 4999 + 4 x 138 follows the order ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async call to Gemini with 5000 prompts in chunks\n",
    "# Uncomment the lines below to retest\n",
    "# sample_dataset_df = await custom_chunked_batch_function_gemini(size=5000)\n",
    "# sample_dataset_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Async call to Gemini with 10000 prompts in chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 10000_prompt_gemini_batch.jsonl\n",
      "Gemini inference time with 10000 prompts: 93.4 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "      <th>async_gemini_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_0</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_1</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_2</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_3</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_4</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_9995</td>\n",
       "      <td>The equation is 9995 + 3 x 144.\\n\\nFollowing t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_9996</td>\n",
       "      <td>Following the order of operations (PEMDAS/BODM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_9997</td>\n",
       "      <td>The result of the equation 9997 + 0 x 646 is *...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_9998</td>\n",
       "      <td>The equation is 9998 + 2 x 431.\\n\\nFollowing t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>{'contents': [{'role': 'user', 'parts': [{'tex...</td>\n",
       "      <td>row_id_9999</td>\n",
       "      <td>The result of the equation 9999 + 0 x 742 is 9...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                request     metadata  \\\n",
       "0     {'contents': [{'role': 'user', 'parts': [{'tex...     row_id_0   \n",
       "1     {'contents': [{'role': 'user', 'parts': [{'tex...     row_id_1   \n",
       "2     {'contents': [{'role': 'user', 'parts': [{'tex...     row_id_2   \n",
       "3     {'contents': [{'role': 'user', 'parts': [{'tex...     row_id_3   \n",
       "4     {'contents': [{'role': 'user', 'parts': [{'tex...     row_id_4   \n",
       "...                                                 ...          ...   \n",
       "9995  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_9995   \n",
       "9996  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_9996   \n",
       "9997  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_9997   \n",
       "9998  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_9998   \n",
       "9999  {'contents': [{'role': 'user', 'parts': [{'tex...  row_id_9999   \n",
       "\n",
       "                                  async_gemini_response  \n",
       "0     Following the order of operations (PEMDAS/BODM...  \n",
       "1     Following the order of operations (PEMDAS/BODM...  \n",
       "2     Following the order of operations (PEMDAS/BODM...  \n",
       "3     Following the order of operations (PEMDAS/BODM...  \n",
       "4     Following the order of operations (PEMDAS/BODM...  \n",
       "...                                                 ...  \n",
       "9995  The equation is 9995 + 3 x 144.\\n\\nFollowing t...  \n",
       "9996  Following the order of operations (PEMDAS/BODM...  \n",
       "9997  The result of the equation 9997 + 0 x 646 is *...  \n",
       "9998  The equation is 9998 + 2 x 431.\\n\\nFollowing t...  \n",
       "9999  The result of the equation 9999 + 0 x 742 is 9...  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Async call to Gemini with 10000 prompts in chunks\n",
    "# Uncomment the lines below to retest\n",
    "# sample_dataset_df = await custom_chunked_batch_function_gemini(size=10000)\n",
    "# sample_dataset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Gemini Batch API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Batch API with GCS as input source and output location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bucket variables defined by user\n",
    "OUTPUT_BUCKET_URI = \"\"  # @param {type:\"string\"}\n",
    "INPUT_BUCKET_URI = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Helper function to submit batch job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataset and upload to GCS bucket\n",
    "def gemini_batch_api_test(size: int) -> BatchPredictionJob:\n",
    "    \"\"\"\n",
    "    Tests the Gemini Batch Prediction API.\n",
    "\n",
    "    This function generates a sample dataset, uploads it to Google Cloud Storage, and submits a batch prediction job to Gemini.\n",
    "\n",
    "    Args:\n",
    "        size (int): The size of the dataset to generate.\n",
    "\n",
    "    Returns:\n",
    "        google.cloud.aiplatform.BatchPredictionJob: The submitted batch prediction job.\n",
    "    \"\"\"\n",
    "    sample_dataset_df, file_name = create_sample_dataset(size=size)\n",
    "\n",
    "    # Upload the dataset to Google Cloud Storage\n",
    "    !gcloud storage cp {file_name} {INPUT_BUCKET_URI}\n",
    "    INPUT_FILE_URI = f\"{INPUT_BUCKET_URI}{file_name}\"\n",
    "\n",
    "    # Submit the batch prediction job\n",
    "    job = BatchPredictionJob.submit(\n",
    "        source_model=DEFAULT_MODEL_NAME,\n",
    "        input_dataset=INPUT_FILE_URI,\n",
    "        output_uri_prefix=OUTPUT_BUCKET_URI,\n",
    "    )\n",
    "\n",
    "    return job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Submitting a Gemini batch job with 5000 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 5000_prompt_gemini_batch.jsonl\n",
      "Copying file://5000_prompt_gemini_batch.jsonl to gs://pemelend_genai_demos/batch/input/5000_prompt_gemini_batch.jsonl\n",
      "  Completed files 1/1 | 1.4MiB/1.4MiB                                          \n",
      "BatchPredictionJob created. Resource name: projects/1054251275628/locations/us-central1/batchPredictionJobs/4746631520045760512\n",
      "To use this BatchPredictionJob in another session:\n",
      "job = batch_prediction.BatchPredictionJob('projects/1054251275628/locations/us-central1/batchPredictionJobs/4746631520045760512')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/4746631520045760512?project=1054251275628\n"
     ]
    }
   ],
   "source": [
    "# Running a job with 5000 prompts\n",
    "job_5k = gemini_batch_api_test(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job resource name: projects/1054251275628/locations/us-central1/batchPredictionJobs/4746631520045760512\n",
      "Model resource name: publishers/google/models/gemini-1.5-flash-002\n",
      "Job state: JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "print(f\"Job resource name: {job_5k.resource_name}\")\n",
    "print(f\"Model resource name: {job_5k.model_name}\")\n",
    "print(f\"Job state: {job_5k.state.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Submitting a Gemini batch job with 10000 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 10000_prompt_gemini_batch.jsonl\n",
      "Copying file://10000_prompt_gemini_batch.jsonl to gs://pemelend_genai_demos/batch/input/10000_prompt_gemini_batch.jsonl\n",
      "  Completed files 1/1 | 2.8MiB/2.8MiB                                          \n",
      "BatchPredictionJob created. Resource name: projects/1054251275628/locations/us-central1/batchPredictionJobs/357873683173212160\n",
      "To use this BatchPredictionJob in another session:\n",
      "job = batch_prediction.BatchPredictionJob('projects/1054251275628/locations/us-central1/batchPredictionJobs/357873683173212160')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/357873683173212160?project=1054251275628\n"
     ]
    }
   ],
   "source": [
    "# Submitting a Gemini batch job with 10000 prompts\n",
    "job_10k = gemini_batch_api_test(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7aJaPNBrGPqK"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job succeeded!\n",
      "Time to complete job: 205.1 seconds\n"
     ]
    }
   ],
   "source": [
    "# Refresh the job until complete\n",
    "while not job_5k.has_ended:\n",
    "    time.sleep(5)\n",
    "    job_5k.refresh()\n",
    "\n",
    "# Check if the job succeeds\n",
    "if job_5k.has_succeeded:\n",
    "    print(\"Job succeeded!\")\n",
    "    print(f\"Time to complete job: {(job_5k.update_time - job_5k.create_time).total_seconds():.1f} seconds\")\n",
    "else:\n",
    "    print(f\"Job failed: {job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Batch API with BigQuery "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset variable defined by user\n",
    "DATASET = \"batch_gemini_source_dataset\"  # @param {type:\"string\"}\n",
    "DATASET_ID = f\"{PROJECT_ID}.{DATASET}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Creating the BigQuery dataset and the table with the JSONL. \n",
    "\n",
    "The batch prediction job and your table must be in the same region (Location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset multi-tenancy-dataproc.batch_gemini_source_dataset\n"
     ]
    }
   ],
   "source": [
    "# Creating a dataset to host the test tables\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Construct a full Dataset object to send to the API.\n",
    "dataset = bigquery.Dataset(DATASET_ID)\n",
    "\n",
    "# TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "dataset.location = LOCATION\n",
    "\n",
    "# Send the dataset to the API for creation, with an explicit timeout.\n",
    "# Raises google.api_core.exceptions.Conflict if the Dataset already\n",
    "# exists within the project.\n",
    "dataset = client.create_dataset(dataset, timeout=30)  # Make an API request.\n",
    "print(f\"Created dataset {PROJECT_ID}.{dataset.dataset_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL file created: 100000_prompt_gemini_batch.jsonl\n",
      "Copying file://100000_prompt_gemini_batch.jsonl to gs://pemelend_genai_demos/batch/input/100000_prompt_gemini_batch.jsonl\n",
      "  Completed files 1/1 | 28.3MiB/28.3MiB                                        \n",
      "\n",
      "Average throughput: 154.7MiB/s\n"
     ]
    }
   ],
   "source": [
    "# Create the sample dataset and upload to GCS\n",
    "size = 100000\n",
    "sample_dataset_df, file_name = create_sample_dataset(size=size)\n",
    "!gcloud storage cp {file_name} {INPUT_BUCKET_URI}\n",
    "INPUT_FILE_URI = f\"{INPUT_BUCKET_URI}{file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table: multi-tenancy-dataproc.batch_gemini_source_dataset.100K_prompt_table and loaded 100000 rows.\n"
     ]
    }
   ],
   "source": [
    "# Create a table in BigQuery to store the data for batch processing.\n",
    "\n",
    "# Set table_id to the ID of the table to create.\n",
    "table_id = f\"{DATASET_ID}.100K_prompt_table\"\n",
    "\n",
    "# Define the schema for the BigQuery table.\n",
    "# The table will have two columns:\n",
    "#   - `request`: JSON column to store the request payload for Gemini.\n",
    "#   - `metadata`: STRING column to store any metadata associated with the request.\n",
    "table_schema = [\n",
    "    {\n",
    "        \"name\": \"request\",\n",
    "        \"type\": \"JSON\",\n",
    "        \"mode\": \"NULLABLE\",\n",
    "        \"maxLength\": \"0\",\n",
    "        \"precision\": \"0\",\n",
    "        \"scale\": \"0\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"metadata\",\n",
    "        \"type\": \"STRING\",\n",
    "        \"mode\": \"NULLABLE\",\n",
    "        \"maxLength\": \"0\",\n",
    "        \"precision\": \"0\",\n",
    "        \"scale\": \"0\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Initialize a BigQuery client.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Configure the load job.\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    schema=table_schema,  # Use the defined schema\n",
    "    source_format=bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,  # Specify the input file format\n",
    ")\n",
    "\n",
    "# Create a load job to load data from the input file URI to the BigQuery table.\n",
    "load_job = client.load_table_from_uri(\n",
    "    INPUT_FILE_URI,  # URI of the input file in Cloud Storage\n",
    "    table_id,  # ID of the table to create\n",
    "    location=\"us-central1\",  # Must match the batch job location\n",
    "    job_config=job_config,  # Use the defined job configuration\n",
    ")\n",
    "\n",
    "# Make an API request and wait for the job to complete.\n",
    "load_job.result()\n",
    "\n",
    "# Get the table details.\n",
    "destination_table = client.get_table(table_id)\n",
    "print(f\"Created table: {table_id} and loaded {destination_table.num_rows} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2309: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2323: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2337: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>request</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>row_id_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>row_id_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>row_id_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>row_id_100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>row_id_1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             request     metadata\n",
       "0  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...     row_id_0\n",
       "1  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...     row_id_1\n",
       "2  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...    row_id_10\n",
       "3  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...   row_id_100\n",
       "4  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...  row_id_1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Querying the table\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "sql = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {table_id}\n",
    "        ORDER BY metadata asc\n",
    "        LIMIT 100\n",
    "        \"\"\"\n",
    "\n",
    "query_result = bq_client.query(sql)\n",
    "\n",
    "df = query_result.result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to create smaller tables used for testing different batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_subset_table(source_table_id, destination_table_id, subset_size):\n",
    "    \"\"\"Creates a subset of a BigQuery table with a specified size.\n",
    "\n",
    "    This function takes a source table, creates a new table with a subset of rows\n",
    "    from the source table, and returns the URI of the new table.\n",
    "\n",
    "    Args:\n",
    "        source_table_id: The ID of the source BigQuery table.\n",
    "        destination_table_id: The ID of the destination BigQuery table.\n",
    "        subset_size: The number of rows to include in the subset table.\n",
    "\n",
    "    Returns:\n",
    "        The table ID and the URI of the created subset table in the format \"bq://{table_id}\".\n",
    "    \"\"\"\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Configure the query job to write the results to the destination table.\n",
    "    job_config = bigquery.QueryJobConfig(destination=destination_table_id)\n",
    "\n",
    "    # Construct the SQL query to select a subset of rows from the source table.\n",
    "    sql = f\"\"\"\n",
    "        SELECT * FROM `{source_table_id}` LIMIT {subset_size}\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the query, passing in the configuration.\n",
    "    query_job = client.query(sql, job_config=job_config)  # Make an API request.\n",
    "    query_job.result()  # Wait for the job to complete.\n",
    "\n",
    "    print(f\"Subset table {destination_table_id} created\")\n",
    "    return destination_table_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset table multi-tenancy-dataproc.batch_gemini_source_dataset.10K_prompt created\n",
      "Subset table multi-tenancy-dataproc.batch_gemini_source_dataset.5K_prompt created\n"
     ]
    }
   ],
   "source": [
    "# Creating smaller tables\n",
    "subset_10k_table_id = create_subset_table(table_id, f\"{DATASET_ID}.10K_prompt\", 10000)\n",
    "subset_5k_table_id = create_subset_table(table_id, f\"{DATASET_ID}.5K_prompt\", 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting a Gemini batch job with 5000 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchPredictionJob created. Resource name: projects/1054251275628/locations/us-central1/batchPredictionJobs/1715005283383640064\n",
      "To use this BatchPredictionJob in another session:\n",
      "job = batch_prediction.BatchPredictionJob('projects/1054251275628/locations/us-central1/batchPredictionJobs/1715005283383640064')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/batch-predictions/1715005283383640064?project=1054251275628\n"
     ]
    }
   ],
   "source": [
    "output_table = f\"{subset_5k_table_id}_gemini_output\"\n",
    "output_uri = f\"bq://{output_table}\"\n",
    "\n",
    "job = BatchPredictionJob.submit(\n",
    "    source_model=DEFAULT_MODEL_NAME, \n",
    "    input_dataset=subset_5k_uri, \n",
    "    output_uri_prefix=output_uri\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2309: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2323: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n",
      "/opt/conda/lib/python3.10/site-packages/google/cloud/bigquery/table.py:2337: UserWarning: Unable to represent RANGE schema as struct using pandas ArrowDtype. Using `object` instead. To use ArrowDtype, use pandas >= 1.5 and pyarrow >= 10.0.1.\n",
      "  warnings.warn(_RANGE_PYARROW_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metadata</th>\n",
       "      <th>status</th>\n",
       "      <th>processed_time</th>\n",
       "      <th>request</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>row_id_595</td>\n",
       "      <td></td>\n",
       "      <td>2024-12-05 00:08:15.991000+00:00</td>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>{\"candidates\":[{\"avgLogprobs\":-0.0210844195551...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>row_id_262</td>\n",
       "      <td></td>\n",
       "      <td>2024-12-05 00:08:17.465000+00:00</td>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>{\"candidates\":[{\"avgLogprobs\":-0.0310279388685...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>row_id_8306</td>\n",
       "      <td></td>\n",
       "      <td>2024-12-05 00:08:11.492000+00:00</td>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>{\"candidates\":[{\"avgLogprobs\":-0.0576387318697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>row_id_675</td>\n",
       "      <td></td>\n",
       "      <td>2024-12-05 00:07:58.707000+00:00</td>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>{\"candidates\":[{\"avgLogprobs\":-0.0204480872434...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>row_id_960</td>\n",
       "      <td></td>\n",
       "      <td>2024-12-05 00:07:57.971000+00:00</td>\n",
       "      <td>{\"contents\":[{\"parts\":[{\"text\":\"Print the resu...</td>\n",
       "      <td>{\"candidates\":[{\"avgLogprobs\":-0.0171888197169...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      metadata status                   processed_time  \\\n",
       "0   row_id_595        2024-12-05 00:08:15.991000+00:00   \n",
       "1   row_id_262        2024-12-05 00:08:17.465000+00:00   \n",
       "2  row_id_8306        2024-12-05 00:08:11.492000+00:00   \n",
       "3   row_id_675        2024-12-05 00:07:58.707000+00:00   \n",
       "4   row_id_960        2024-12-05 00:07:57.971000+00:00   \n",
       "\n",
       "                                             request  \\\n",
       "0  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...   \n",
       "1  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...   \n",
       "2  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...   \n",
       "3  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...   \n",
       "4  {\"contents\":[{\"parts\":[{\"text\":\"Print the resu...   \n",
       "\n",
       "                                            response  \n",
       "0  {\"candidates\":[{\"avgLogprobs\":-0.0210844195551...  \n",
       "1  {\"candidates\":[{\"avgLogprobs\":-0.0310279388685...  \n",
       "2  {\"candidates\":[{\"avgLogprobs\":-0.0576387318697...  \n",
       "3  {\"candidates\":[{\"avgLogprobs\":-0.0204480872434...  \n",
       "4  {\"candidates\":[{\"avgLogprobs\":-0.0171888197169...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "sql = f\"\"\"\n",
    "        SELECT *\n",
    "        FROM {output_table}\n",
    "        where metadata > \"row_id_10009\"\n",
    "        LIMIT 100\n",
    "        \"\"\"\n",
    "\n",
    "query_result = bq_client.query(sql)\n",
    "\n",
    "df = query_result.result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following the order of operations (PEMDAS/BODMAS), multiplication comes before addition.  Therefore:\n",
      "\n",
      "1. **Multiplication:** 4 x 21 = 84\n",
      "\n",
      "2. **Addition:** 1136 + 84 = 1220\n",
      "\n",
      "Therefore, the result of the equation 1136 + 4 x 21 is $\\boxed{1220}$.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Gemini's response\n",
    "print(json.loads(df.iloc[23].response)['candidates'][0]['content']['parts'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Custom batch sending asyncronous calls to Online Gemini\n",
    "\n",
    "* Pros:\n",
    "    - Full flexibility and control of the data pipeline\n",
    "    - No size limitation\n",
    "    - It could be faster than batch API \n",
    "    - More predictable performance, specially with Provision Throughput\n",
    "<br><br>\n",
    "\n",
    "* Cons:\n",
    "    - Higher cost \n",
    "    - Scripting can get complicated\n",
    "    - Quota issues\n",
    "\n",
    "\n",
    "\n",
    "### Using Gemini Batch API:\n",
    "        \n",
    "* Pros:\n",
    "    - Simplified API. Easy to run job.\n",
    "    - No custom scripting required\n",
    "    - Lower cost \n",
    "    - Quotas abstraction\n",
    "    - SQL interface with BigQuery (Easier to control dataset with SQL commands)\n",
    "\n",
    "    \n",
    "* Cons:\n",
    "    - Harder to predict time to execute\n",
    "    - Size limitations. Maximum of 1M prompts per batch is recommended\n",
    "    - It might be slower than online\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "- Remove the BigQuery dataset\n",
    "- Delete files from GCS or delete the bucket. \n",
    "\n",
    "**Caution: [Soft delete](https://cloud.google.com/storage/docs/soft-delete) is enabled on Cloud Storage buckets, which retain content in a deleted state, potentially resulting in significant storage costs. Disable soft delete to prevent unexpected charges.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment the line below to delete the BigQuery dataset\n",
    "# !bq rm -r -f -d {DATASET_ID.replace('.',':')}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
